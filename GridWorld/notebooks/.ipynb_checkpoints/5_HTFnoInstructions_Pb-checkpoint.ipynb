{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3da09b5",
   "metadata": {},
   "source": [
    "# Experiments RLHTF\n",
    "#### Nov. 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10cbd5c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca81719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set notebook up to load files from Science repo\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Ensure that we re-load changes automagically\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9cc4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e22499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3202cbd-f567-4477-85d4-a5171bf7d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECRET_KEY = TO ADD API KEY FOR GPT4.0 ACCESS\n",
    "import openai\n",
    "openai.api_key = SECRET_KEY\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df206cd5",
   "metadata": {},
   "source": [
    "### Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5da176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from science.agents import GridEnvironment,  QLearningAgent_Bernoulli, QLearningAgent_Bernoulli_greedy, QLearningAgent_Bernoulli_PbRL\n",
    "from science.draw_map import create_grid_map, add_trajectory, map_reward_estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f00696-982c-4a7f-bab3-30bd645980b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from science.data_to_GPT import load_experiments, get_experiment_dataframe, calculate_deviation, plot_mean_scores_bars\n",
    "from science.data_to_GPT import algebraic_to_index, create_system_message, get_reward_function_structure\n",
    "from science.data_to_GPT import create_system_message_with_uncertainty, get_reward_function_structure_with_uncertainty\n",
    "from science.data_to_GPT import create_system_message_immediate, get_reward_function_structure_immediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c472d110-7eea-4e02-acd0-a1b3f9fee9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from science.feedback import sample_reward_HF_GPT_nocertainty_v2\n",
    "from science.feedback_pairwise import ask_preference_HF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f636d1e2",
   "metadata": {},
   "source": [
    "## Initialize the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6061227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from science.config import grid_width, grid_height, NUM_STEPS\n",
    "car_init = [[0, 0]]\n",
    "num_iters_per_experiment = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a18594",
   "metadata": {},
   "source": [
    "## Load Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f43dc-7ce8-45e2-8033-70736df0b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 10\n",
    "experiment_options = []\n",
    "# Define the directories\n",
    "directory = \"./setup/\"\n",
    "file_basename = \"exp_init\"\n",
    "\n",
    "for i in range(1, num_experiments+1):\n",
    "    filename = f\"{directory}{file_basename}{i}.pkl\"\n",
    "    try:\n",
    "        with open(filename, \"rb\") as file:\n",
    "            # Placeholders for contents that are not used here\n",
    "            experiment_options.append(pickle.load(file))\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"The file {filename} was not found.\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"An error occurred while loading {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9bf14d",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd92dc",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_human = \"test1\"\n",
    "date = \"dec11\"\n",
    "experiment_order =  list(range(0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca77d49c",
   "metadata": {},
   "source": [
    "###  Consent\n",
    "\n",
    "You are being asked to be a volunteer in a research study.  The purpose of this study is to improve the training of artificial intelligence (AI) systems by using text feedback from humans. The interactive task will take approximately 30 minutes to complete.  Your confidentiality will be protected by assigning anonymized IDs and ensuring that no identifiable information is collected or stored. The risks involved are no greater than those involved in daily activities.  You will not benefit or be compensated for joining this study. We will comply with any applicable laws and regulations regarding confidentiality. To make sure that this research is being carried out in the proper way, the IRB may review study records.  The Office of Human Research Protections may also look at study records.   If you have any questions about the study, you may contact (deleted for anonymity in review process). If you have any questions about your rights as a research subject, you may contact (deleted for anonymity  in review process). Thank you for participating in this study.\n",
    "\t\n",
    "\t\n",
    "**By completing the survey, you indicate your consent to be in the study.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a185f330",
   "metadata": {},
   "source": [
    "### Part 1/2: Instructions\n",
    "\n",
    "Thank you for helping out with the experiment!\n",
    "\n",
    "**Objective**: \n",
    "The purpose of this experiment is to provide instructional feedback to an artificial agent. The task for the agent is to navigate rom a yellow circle to a yellow star along a gray pathway. The agent will attempt this task by following a trajectory marked in blue. Your role is to offer written feedback that assists in correcting the agent's current course.\n",
    "\n",
    "**Task Instructions**:\n",
    "\n",
    "1. Observe the blue trajectory that the agent has taken.\n",
    "2. Provide your guidance and feedback on the agent's performance (e.g.: \"Do not go below the sofa. The end was very good\")\n",
    "3. Repeat 4 times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677d2112",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2273a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[images, loc_landmarks, road, pixel_landmarks, list_landmarks] = experiment_options[8]\n",
    "# Initialize the Map\n",
    "locations = car_init + loc_landmarks # Coordinates for starting point of car + landmarks\n",
    "# Define Grid and RL Agent\n",
    "env = GridEnvironment(grid_width, grid_height, NUM_STEPS, car_init[0])\n",
    "agent_Bernoulli = QLearningAgent_Bernoulli_greedy(env, alpha_init = 0.5, beta_init = 0.5)\n",
    "# Run learning\n",
    "reward_Bernoulli = agent_Bernoulli.learn(2, sample_reward_HF_GPT_nocertainty_v2,\\\n",
    "                                         images, loc_landmarks, road, grid_width, grid_height, car_init[0], \\\n",
    "                                        pixel_landmarks, list_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7147226a",
   "metadata": {},
   "source": [
    "### 1/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9a53fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_exp = experiment_order[0]\n",
    "[images, loc_landmarks, road, pixel_landmarks, list_landmarks] = experiment_options[index_exp]\n",
    "name_experiment = \"RLHTFNoInstruct_1_exp\" + str(index_exp) + \".pkl\"\n",
    "# Initialize the Map\n",
    "locations = car_init + loc_landmarks\n",
    "# Define Grid and RL Agent\n",
    "env = GridEnvironment(grid_width, grid_height, NUM_STEPS, car_init[0])\n",
    "agent_Bernoulli = QLearningAgent_Bernoulli_greedy(env, alpha_init = 0.5, beta_init = 0.5)\n",
    "# Run learning\n",
    "reward_Bernoulli = agent_Bernoulli.learn(num_iters_per_experiment, sample_reward_HF_GPT_nocertainty_v2,\\\n",
    "                                         images, loc_landmarks, road, grid_width, grid_height, car_init[0], \\\n",
    "                                        pixel_landmarks, list_landmarks)\n",
    "optimal_trajectory_Bernoulli = agent_Bernoulli.get_optimal_trajectory()\n",
    "# Save Experiment Data\n",
    "agent_Bernoulli.save_experiment(name_experiment, name_human, date, optimal_trajectory_Bernoulli, road)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc0f1cd",
   "metadata": {},
   "source": [
    "### 2/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a98669",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_exp = experiment_order[1]\n",
    "[images, loc_landmarks, road, pixel_landmarks, list_landmarks] = experiment_options[index_exp]\n",
    "name_experiment = \"RLHTFNoInstruct_1_exp\" + str(index_exp) + \".pkl\"\n",
    "# Initialize the Map\n",
    "locations = car_init + loc_landmarks # Coordinates for starting point of car + landmarks\n",
    "# Define Grid and RL Agent\n",
    "env = GridEnvironment(grid_width, grid_height, NUM_STEPS, car_init[0])\n",
    "agent_Bernoulli =QLearningAgent_Bernoulli_greedy(env, alpha_init = 0.5, beta_init = 0.5)\n",
    "# Run learning\n",
    "reward_Bernoulli = agent_Bernoulli.learn(num_iters_per_experiment, sample_reward_HF_GPT_nocertainty_v2,\\\n",
    "                                         images, loc_landmarks, road, grid_width, grid_height, car_init[0], \\\n",
    "                                        pixel_landmarks, list_landmarks)\n",
    "optimal_trajectory_Bernoulli = agent_Bernoulli.get_optimal_trajectory()\n",
    "# Save Experiment Data\n",
    "agent_Bernoulli.save_experiment(name_experiment, name_human, date, optimal_trajectory_Bernoulli, road)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73795dd8",
   "metadata": {},
   "source": [
    "### 3/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cf7f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_exp = experiment_order[2]\n",
    "[images, loc_landmarks, road, pixel_landmarks, list_landmarks] = experiment_options[index_exp]\n",
    "name_experiment = \"RLHTFNoInstruct_1_exp\" + str(index_exp) + \".pkl\"\n",
    "# Initialize the Map\n",
    "locations = car_init + loc_landmarks # Coordinates for starting point of car + landmarks\n",
    "# Define Grid and RL Agent\n",
    "env = GridEnvironment(grid_width, grid_height, NUM_STEPS, car_init[0])\n",
    "agent_Bernoulli = QLearningAgent_Bernoulli_greedy(env, alpha_init = 0.5, beta_init = 0.5)\n",
    "# Run learning\n",
    "reward_Bernoulli = agent_Bernoulli.learn(num_iters_per_experiment, sample_reward_HF_GPT_nocertainty_v2,\\\n",
    "                                         images, loc_landmarks, road, grid_width, grid_height, car_init[0], \\\n",
    "                                        pixel_landmarks, list_landmarks)\n",
    "optimal_trajectory_Bernoulli = agent_Bernoulli.get_optimal_trajectory()\n",
    "# Save Experiment Data\n",
    "agent_Bernoulli.save_experiment(name_experiment, name_human, date, optimal_trajectory_Bernoulli, road)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b21885",
   "metadata": {},
   "source": [
    "### 4/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f9231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_exp = experiment_order[3]\n",
    "[images, loc_landmarks, road, pixel_landmarks, list_landmarks] = experiment_options[index_exp]\n",
    "name_experiment = \"RLHTFNoInstruct_1_exp\" + str(index_exp) + \".pkl\"\n",
    "# Initialize the Map\n",
    "locations = car_init + loc_landmarks # Coordinates for starting point of car + landmarks\n",
    "# Define Grid and RL Agent\n",
    "env = GridEnvironment(grid_width, grid_height, NUM_STEPS, car_init[0])\n",
    "agent_Bernoulli = QLearningAgent_Bernoulli_greedy(env, alpha_init = 0.5, beta_init = 0.5)\n",
    "# Run learning\n",
    "reward_Bernoulli = agent_Bernoulli.learn(num_iters_per_experiment, sample_reward_HF_GPT_nocertainty_v2,\\\n",
    "                                         images, loc_landmarks, road, grid_width, grid_height, car_init[0], \\\n",
    "                                        pixel_landmarks, list_landmarks)\n",
    "optimal_trajectory_Bernoulli = agent_Bernoulli.get_optimal_trajectory()\n",
    "# Save Experiment Data\n",
    "agent_Bernoulli.save_experiment(name_experiment, name_human, date, optimal_trajectory_Bernoulli, road)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a77111",
   "metadata": {},
   "source": [
    "### Part 2/2: Instructions\n",
    "\n",
    "Thank you for compleating the experiments with text feedback! (That was the longest part, I promise you are almost done!)\n",
    "\n",
    "For the second part of the experiment, two agents will attempt this task by following a trajectory marked in blue and red respectively. Your role is to select which of the two trajectories is better, so that the agents can learn the correct path.\n",
    "\n",
    "**Task Instructions**:\n",
    "\n",
    "1. Observe the blue and red trajectories that the agents have taken.\n",
    "2. Chose the best one (0: blue, 1: red).\n",
    "3. Repeat 4 times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b221d39",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022ec834-b400-47c4-ba05-89934c5ec322",
   "metadata": {},
   "outputs": [],
   "source": [
    "[images, loc_landmarks, road, pixel_landmarks, list_landmarks] = experiment_options[experiment_order[2]]#index_exp]\n",
    "# Initialize the Map\n",
    "locations = car_init + loc_landmarks # Coordinates for starting point of car + landmarks\n",
    "# Define Grid and RL Agent\n",
    "env = GridEnvironment(grid_width, grid_height, NUM_STEPS, car_init[0])\n",
    "agent_Bernoulli = QLearningAgent_Bernoulli_PbRL(env, alpha_init = 0.5, beta_init = 0.5)\n",
    "# Run learning\n",
    "reward_Bernoulli = agent_Bernoulli.learn(1, ask_preference_HF,\\\n",
    "                                         road, grid_width, grid_height, car_init[0], num_samples=20, mode = 'greedy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b985644",
   "metadata": {},
   "source": [
    "### 1/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a74e836",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index_exp = experiment_order[4]\n",
    "[images, loc_landmarks, road, pixel_landmarks, list_landmarks] = experiment_options[index_exp]\n",
    "name_experiment = \"PbRL_2_exp\" + str(index_exp) + \".pkl\"\n",
    "# Initialize the Map\n",
    "locations = car_init + loc_landmarks # Coordinates for starting point of car + landmarks\n",
    "# Define Grid and RL Agent\n",
    "env = GridEnvironment(grid_width, grid_height, NUM_STEPS, car_init[0])\n",
    "agent_Bernoulli = QLearningAgent_Bernoulli_PbRL(env, alpha_init = 0.5, beta_init = 0.5)\n",
    "# Run learning\n",
    "reward_Bernoulli = agent_Bernoulli.learn(num_iters_per_experiment, ask_preference_HF,\\\n",
    "                                         road, grid_width, grid_height, car_init[0], num_samples=20, mode = 'greedy')\n",
    "optimal_trajectory_Bernoulli = agent_Bernoulli.get_optimal_trajectory()\n",
    "# Save Experiment Data\n",
    "agent_Bernoulli.save_experiment(name_experiment, name_human, date, optimal_trajectory_Bernoulli, road)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed39ee5",
   "metadata": {},
   "source": [
    "### 2/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3cdbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_exp = experiment_order[5]\n",
    "[images, loc_landmarks, road, pixel_landmarks, list_landmarks] = experiment_options[index_exp]\n",
    "name_experiment = \"PbRL_2_exp\" + str(index_exp) + \".pkl\"\n",
    "# Initialize the Map\n",
    "locations = car_init + loc_landmarks # Coordinates for starting point of car + landmarks\n",
    "# Define Grid and RL Agent\n",
    "env = GridEnvironment(grid_width, grid_height, NUM_STEPS, car_init[0])\n",
    "agent_Bernoulli = QLearningAgent_Bernoulli_PbRL(env, alpha_init = 0.5, beta_init = 0.5)\n",
    "# Run learning\n",
    "reward_Bernoulli = agent_Bernoulli.learn(num_iters_per_experiment, ask_preference_HF,\\\n",
    "                                         road, grid_width, grid_height, car_init[0], num_samples=20, mode = 'greedy')\n",
    "optimal_trajectory_Bernoulli = agent_Bernoulli.get_optimal_trajectory()\n",
    "# Save Experiment Data\n",
    "agent_Bernoulli.save_experiment(name_experiment, name_human, date, optimal_trajectory_Bernoulli, road)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591512e",
   "metadata": {},
   "source": [
    "### 3/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1010e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_exp = experiment_order[6]\n",
    "[images, loc_landmarks, road, pixel_landmarks, list_landmarks] = experiment_options[index_exp]\n",
    "name_experiment = \"PbRL_2_exp\" + str(index_exp) + \".pkl\"\n",
    "# Initialize the Map\n",
    "locations = car_init + loc_landmarks # Coordinates for starting point of car + landmarks\n",
    "# Define Grid and RL Agent\n",
    "env = GridEnvironment(grid_width, grid_height, NUM_STEPS, car_init[0])\n",
    "agent_Bernoulli = QLearningAgent_Bernoulli_PbRL(env, alpha_init = 0.5, beta_init = 0.5)\n",
    "# Run learning\n",
    "reward_Bernoulli = agent_Bernoulli.learn(num_iters_per_experiment, ask_preference_HF,\\\n",
    "                                         road, grid_width, grid_height, car_init[0], num_samples=20, mode = 'greedy')\n",
    "optimal_trajectory_Bernoulli = agent_Bernoulli.get_optimal_trajectory()\n",
    "# Save Experiment Data\n",
    "agent_Bernoulli.save_experiment(name_experiment, name_human, date, optimal_trajectory_Bernoulli, road)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d93a06",
   "metadata": {},
   "source": [
    "### 4/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c17edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_exp = experiment_order[7]\n",
    "[images, loc_landmarks, road, pixel_landmarks, list_landmarks] = experiment_options[index_exp]\n",
    "name_experiment = \"PbRL_2_exp\" + str(index_exp) + \".pkl\"\n",
    "# Initialize the Map\n",
    "locations = car_init + loc_landmarks # Coordinates for starting point of car + landmarks\n",
    "# Define Grid and RL Agent\n",
    "env = GridEnvironment(grid_width, grid_height, NUM_STEPS, car_init[0])\n",
    "agent_Bernoulli = QLearningAgent_Bernoulli_PbRL(env, alpha_init = 0.5, beta_init = 0.5)\n",
    "# Run learning\n",
    "reward_Bernoulli = agent_Bernoulli.learn(num_iters_per_experiment, ask_preference_HF,\\\n",
    "                                         road, grid_width, grid_height, car_init[0], num_samples=20, mode = 'greedy')\n",
    "optimal_trajectory_Bernoulli = agent_Bernoulli.get_optimal_trajectory()\n",
    "# Save Experiment Data\n",
    "agent_Bernoulli.save_experiment(name_experiment, name_human, date, optimal_trajectory_Bernoulli, road)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a408809",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
