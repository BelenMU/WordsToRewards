{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3da09b5",
   "metadata": {},
   "source": [
    "# Experiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10cbd5c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72bae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68803121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b05dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set notebook up to load files from Science repo\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Ensure that we re-load changes automagically\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169a7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from science.agents import GridEnvironment, QLearningAgent_Bernoulli_greedy, QLearningAgent_Bernoulli_PbRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53470608",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters_per_experiment = 4\n",
    "grid_height = 5\n",
    "grid_width = 10\n",
    "num_steps = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7529c7",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8674a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory where the pkl files are located\n",
    "base_dir = 'human_experiments'\n",
    "\n",
    "# List all .pkl files in the human_experiments directory\n",
    "pkl_files = glob.glob(os.path.join(base_dir, 'nov*_*_*.pkl')) + glob.glob(os.path.join(base_dir, 'dec*_*_*.pkl'))\n",
    "\n",
    "# Regex to extract day, participant, order, potential version, and experiment number\n",
    "pattern = r'(nov|dec)(\\d+)_postirb(\\d+)_([a-z]+)_(\\d+)_exp(\\d+)\\.pkl'\n",
    "\n",
    "# To store the loaded data along with their metadata\n",
    "experiments_data = []\n",
    "\n",
    "# Loop over each .pkl file\n",
    "for file_path in pkl_files:\n",
    "    # Extract the metadata from the filename using regex\n",
    "    filename = os.path.basename(file_path)\n",
    "    match = re.match(pattern, filename.lower())\n",
    "    if match:\n",
    "        month, day, participant, experiment_type, order, experiment_number = match.groups()\n",
    "        # Load the .pkl file\n",
    "        data = pd.read_pickle(file_path)\n",
    "        participant = int(participant)\n",
    "        experiment_number = int(experiment_number)\n",
    "        # Calculate deviations for each trajectory\n",
    "        road = data['road'][1:]\n",
    "        if experiment_type == 'rlhtf':\n",
    "            deviations = [calculate_deviation(trajectory, road) for trajectory in data['trajectory']]\n",
    "        else:\n",
    "            deviations = data['reward']\n",
    "        final_score = calculate_deviation(data['learned_trajectory'], road)\n",
    "        \n",
    "        # Append a tuple of the metadata and data to the experiments_data list\n",
    "        experiments_data.append((month, day, participant, experiment_type, order, experiment_number, data, deviations, final_score))\n",
    "    else:\n",
    "        print(f\"Filename {filename} did not match the pattern and was skipped.\")\n",
    "# Convert the list to a DataFrame\n",
    "experiments_df = pd.DataFrame(experiments_data, columns=['Month', 'Day', 'participant', 'ExperimentType', 'AgentType', 'ExperimentNumber', 'Data', 'Deviation', 'Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcaeb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = experiments_df[\n",
    "    experiments_df['Data'].apply(lambda data: len(data['trajectory']) == 4)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a94b5bd",
   "metadata": {},
   "source": [
    "## Compute Experiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e87ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_deviation(trajectory, road):   \n",
    "    deviation = 0\n",
    "    for step in trajectory:\n",
    "        if not any(np.array_equal(step, road_step) for road_step in road):\n",
    "            deviation -= 1\n",
    "    return 10+deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_computed_scores(row):\n",
    "    index = row.name  # Get the index of the row\n",
    "    data_entry = experiments_df['Data'][index]\n",
    "    \n",
    "    # Extract road and trajectories\n",
    "    road = data_entry['road']\n",
    "    trajectories = data_entry['trajectory']  # Assuming it's a list of four trajectories\n",
    "    learned_trajectory = data_entry['learned_trajectory']  # The additional trajectory\n",
    "    \n",
    "    # Compute deviations\n",
    "    scores = [calculate_deviation(traj, road) for traj in trajectories]\n",
    "    learned_score = calculate_deviation(learned_trajectory, road)\n",
    "    \n",
    "    # Append the learned score\n",
    "    scores.append(learned_score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0581e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safely assigning the new column\n",
    "filtered_df = filtered_df.copy()  # Ensure it's a separate copy to avoid the warning\n",
    "\n",
    "filtered_df['ComputedScore'] = filtered_df.apply(calculate_computed_scores, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44eacdc",
   "metadata": {},
   "source": [
    "#### Deviations for pbrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036c9b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_complete = filtered_df[(filtered_df['ExperimentType'] == 'pbrl')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3427bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "deviation_PbRL = np.zeros([len(to_complete), 5])\n",
    "for ee in range(len(to_complete)):\n",
    "    road = to_complete['Data'].iloc[ee]['road'][1:]\n",
    "    env = GridEnvironment(grid_width, grid_height, num_steps, [0,0])\n",
    "    agent_Bernoulli = QLearningAgent_Bernoulli_PbRL(env, alpha_init = 0.5, beta_init = 0.5)\n",
    "    trajectory = agent_Bernoulli.get_optimal_trajectory()\n",
    "    deviation_PbRL[ee, 0] = calculate_deviation(trajectory, road)\n",
    "    for ii in range(num_iters_per_experiment):\n",
    "        traj1 = to_complete['Data'].iloc[ee]['trajectory'][ii][0]\n",
    "        traj2 = to_complete['Data'].iloc[ee]['trajectory'][ii][1]\n",
    "        human_feedback = to_complete['Data'].iloc[ee]['human_feedback'][ii]\n",
    "            \n",
    "        # Update alpha and beta based on feedback\n",
    "        for state1, state2 in zip(traj1, traj2):\n",
    "            idx1 = agent_Bernoulli.state_to_index(state1)\n",
    "            idx2 = agent_Bernoulli.state_to_index(state2)\n",
    "\n",
    "            if idx1 != idx2:\n",
    "                if human_feedback == 0: # Trajectory 1 is prefered\n",
    "                    agent_Bernoulli.alpha[idx1] += agent_Bernoulli.scale\n",
    "                    agent_Bernoulli.beta[idx2] += agent_Bernoulli.scale\n",
    "                else: # Trajectory 2 is prefered\n",
    "                    agent_Bernoulli.beta[idx1] += agent_Bernoulli.scale\n",
    "                    agent_Bernoulli.alpha[idx2] += agent_Bernoulli.scale\n",
    "        trajectory = agent_Bernoulli.get_optimal_trajectory()\n",
    "        #print(f\"trajectory: {trajectory}\")\n",
    "        deviation_PbRL[ee, ii+1] = calculate_deviation(trajectory, road)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a26a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices where ['ExperimentType'] == 'pbrl'\n",
    "pbrl_indices = filtered_df[(filtered_df['ExperimentType'] == 'pbrl')].index\n",
    "for i, idx in enumerate(pbrl_indices):\n",
    "    filtered_df.at[idx, 'ComputedScore'] = deviation_PbRL[i].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e4c33",
   "metadata": {},
   "source": [
    "## Experiment Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd62ce12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the summary table\n",
    "summary_table = (\n",
    "    filtered_df\n",
    "    .groupby(['ExperimentType', 'AgentType'])\n",
    "    .size()  # Count occurrences\n",
    "    .unstack(fill_value=0)  # Make columns for each 'AgentType', filling missing values with 0\n",
    "    .rename(columns={1: 'Count_AgentType_1', 2: 'Count_AgentType_2'})  # Rename columns\n",
    ")\n",
    "\n",
    "# Add total counts for each 'ExperimentType'\n",
    "summary_table['Total'] = summary_table.sum(axis=1)\n",
    "\n",
    "# Reset the index to turn it into a standard table\n",
    "summary_table = summary_table.reset_index()\n",
    "\n",
    "print(summary_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed16f43",
   "metadata": {},
   "source": [
    "## Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab0ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sentiment = filtered_df[(filtered_df['ExperimentType'] == 'sentiment')]['ComputedScore']\n",
    "score_pbrl = filtered_df[(filtered_df['ExperimentType'] == 'pbrl')]['ComputedScore']\n",
    "score_rlhtfnoinstruct = filtered_df[(filtered_df['ExperimentType'] == 'rlhtfnoinstruct')]['ComputedScore']\n",
    "score_rlhtf = filtered_df[(filtered_df['ExperimentType'] == 'rlhtf')]['ComputedScore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58725da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(score_sentiment.values.tolist())\n",
    "score_sentiment_mean = df.mean(axis=0).tolist()\n",
    "score_sentiment_ste = (df.std(axis=0) / np.sqrt(len(df))).tolist()\n",
    "\n",
    "df = pd.DataFrame(score_pbrl.values.tolist())\n",
    "score_pbrl_mean = df.mean(axis=0).tolist()\n",
    "score_pbrl_ste = (df.std(axis=0) / np.sqrt(len(df))).tolist()\n",
    "\n",
    "df = pd.DataFrame(score_rlhtfnoinstruct.values.tolist())\n",
    "score_rlhtfnoinstruct_mean = df.mean(axis=0).tolist()\n",
    "score_rlhtfnoinstruct_ste = (df.std(axis=0) / np.sqrt(len(df))).tolist()\n",
    "\n",
    "df = pd.DataFrame(score_rlhtf.values.tolist())\n",
    "score_rlhtf_mean = df.mean(axis=0).tolist()\n",
    "score_rlhtf_ste = (df.std(axis=0) / np.sqrt(len(df))).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8932cdff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('human_experiments/grid_true_trajectory_greedy.pickle', 'rb') as f:\n",
    "    reward_true_UCB_trajectory = pickle.load(f)\n",
    "with open('human_experiments/grid_true_state_greedy.pickle', 'rb') as f:\n",
    "    reward_true_UCB_state = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032009b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the means and standard errors (replace these with the values you computed)\n",
    "means = {\n",
    "    'score_sentiment': score_sentiment_mean,\n",
    "    'score_pbrl': score_pbrl_mean,\n",
    "    'score_rlhtfnoinstruct': score_rlhtfnoinstruct_mean,\n",
    "    'score_rlhtf': score_rlhtf_mean\n",
    "}\n",
    "\n",
    "stes = {\n",
    "    'score_sentiment': score_sentiment_ste,\n",
    "    'score_pbrl': score_pbrl_ste,\n",
    "    'score_rlhtfnoinstruct': score_rlhtfnoinstruct_ste,\n",
    "    'score_rlhtf': score_rlhtf_ste\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5566bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "df_rlhtf = pd.DataFrame(score_rlhtf.values.tolist())\n",
    "df_pbrl = pd.DataFrame(score_pbrl.values.tolist())\n",
    "df_rlhtfnoinstruct = pd.DataFrame(score_rlhtfnoinstruct.values.tolist())\n",
    "df_sentiment = pd.DataFrame(score_sentiment.values.tolist())\n",
    "df_true_state = pd.DataFrame(reward_true_UCB_state)\n",
    "df_true_trajectory = pd.DataFrame(reward_true_UCB_trajectory)\n",
    "\n",
    "# Compute error (10 - score)\n",
    "error_rlhtf = 10 - df_rlhtf\n",
    "error_pbrl = 10 - df_pbrl\n",
    "error_rlhtfnoinstruct = 10 - df_rlhtfnoinstruct\n",
    "error_sentiment = 10 - df_sentiment\n",
    "error_true_state  = - df_true_state\n",
    "error_true_trajectory  = - df_true_trajectory \n",
    "\n",
    "# Extract the last column (final step)\n",
    "last_step_error_rlhtf = error_rlhtf.iloc[:, -1]  # Last column\n",
    "last_step_error_pbrl = error_pbrl.iloc[:, -1]    # Last column\n",
    "last_step_error_rlhtfnoinstruct = error_rlhtfnoinstruct.iloc[:, -1] \n",
    "last_step_error_sentiment = error_sentiment.iloc[:, -1]  # Last column\n",
    "last_step_error_true_state = error_true_state.iloc[:, -1]  \n",
    "last_step_error_true_trajectory = error_true_trajectory.iloc[:, -1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb6fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Welch's t-test (independent t-test with unequal variances)\n",
    "t_stat, p_value = ttest_ind(last_step_error_rlhtf, last_step_error_true_state, equal_var=False)\n",
    "print(f\"RLHTF & true state: T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29267efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Welch's t-test (independent t-test with unequal variances)\n",
    "t_stat, p_value = ttest_ind(last_step_error_rlhtf, last_step_error_true_trajectory, equal_var=False)\n",
    "print(f\"RLHTF & true traj: T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd368bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Welch's t-test (independent t-test with unequal variances)\n",
    "t_stat, p_value = ttest_ind(last_step_error_rlhtfnoinstruct, last_step_error_true_state, equal_var=False)\n",
    "print(f\"RLHTFnoinstruct & true state: T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b04c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Welch's t-test (independent t-test with unequal variances)\n",
    "t_stat, p_value = ttest_ind(last_step_error_rlhtfnoinstruct, last_step_error_true_trajectory, equal_var=False)\n",
    "print(f\"RLHTF noinstruct& true traj: T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Welch's t-test (independent t-test with unequal variances)\n",
    "t_stat, p_value = ttest_ind(last_step_error_pbrl, last_step_error_true_state, equal_var=False)\n",
    "print(f\"pbrl & true state: T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6de6540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Welch's t-test (independent t-test with unequal variances)\n",
    "t_stat, p_value = ttest_ind(last_step_error_sentiment, last_step_error_true_trajectory, equal_var=False)\n",
    "print(f\"sentiment & true traj: T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49cf32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Welch's t-test (independent t-test with unequal variances)\n",
    "t_stat, p_value = ttest_ind(last_step_error_sentiment, last_step_error_true_state, equal_var=False)\n",
    "print(f\"sentiment & true state: T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967830c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Welch's t-test (independent t-test with unequal variances)\n",
    "t_stat, p_value = ttest_ind(last_step_error_true_trajectory, last_step_error_true_state, equal_var=False)\n",
    "print(f\"true trajectory & true state: T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2ebaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Welch's t-test (independent t-test with unequal variances)\n",
    "t_stat, p_value = ttest_ind(last_step_error_pbrl, last_step_error_true_trajectory, equal_var=False)\n",
    "print(f\"pbrl & true traj: T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461b395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Welch's t-test (independent t-test with unequal variances)\n",
    "t_stat, p_value = ttest_ind(last_step_error_rlhtf, last_step_error_pbrl, equal_var=False)\n",
    "print(f\"RLHTF & PbRL: T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb03d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Welch's t-test (independent t-test with unequal variances)\n",
    "t_stat, p_value = ttest_ind(last_step_error_sentiment, last_step_error_pbrl, equal_var=False)\n",
    "print(f\"sentiment & PbRL: T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_stat, p_value = ttest_ind(last_step_error_rlhtfnoinstruct, last_step_error_pbrl, equal_var=False)\n",
    "print(f\"RLHTF No Instructions & PbRL: T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45238a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_stat, p_value = ttest_ind(last_step_error_rlhtfnoinstruct, last_step_error_sentiment, equal_var=False)\n",
    "print(f\"RLHTF No Instructions & sentiment: T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639539c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Welch's t-test (independent t-test with unequal variances)\n",
    "t_stat, p_value = ttest_ind(last_step_error_rlhtf, last_step_error_sentiment, equal_var=False)\n",
    "print(f\"RLHTF & sentiment: T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633d05ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_stat, p_value = ttest_ind(last_step_error_rlhtf, last_step_error_rlhtfnoinstruct,equal_var=False)\n",
    "print(f\"RLHTF & RLHTF No Instructions: T-statistic: {t_stat}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b33ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load the variables from the file\n",
    "with open('true_trajectory_vs_state.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "# Access the loaded variables\n",
    "deviation_mean_true_state = loaded_data['deviation_mean_true_state']\n",
    "deviation_ste_true_state = loaded_data['deviation_ste_true_state']\n",
    "deviation_mean_true_traj = loaded_data['deviation_mean_true_traj']\n",
    "deviation_ste_true_traj = loaded_data['deviation_ste_true_traj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27844df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the x-axis positions for the bars\n",
    "x = np.arange(5)  # 5 steps\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Width of the bars\n",
    "bar_width = 0.15\n",
    "\n",
    "# Position offsets for the bars\n",
    "positions = {\n",
    "    'true_traj': x - 2.5 * bar_width,\n",
    "    'true_state': x - 1.5 * bar_width,\n",
    "    'score_sentiment': x - 0.5 * bar_width,\n",
    "    'score_pbrl': x + 0.5 * bar_width,\n",
    "    'score_rlhtfnoinstruct': x + 1.5 * bar_width,\n",
    "    'score_rlhtf': x + 2.5 * bar_width\n",
    "}\n",
    "\n",
    "ax.bar(positions['true_traj'], -deviation_mean_true_traj,\n",
    "       bar_width,\n",
    "       label='True trajectory level',\n",
    "       color='#d62728',\n",
    "       alpha=0.8,\n",
    "       yerr=deviation_ste_true_traj,\n",
    "       capsize=5)\n",
    "\n",
    "ax.bar(positions['true_state'], -deviation_mean_true_state,\n",
    "       bar_width,\n",
    "       label='True state level',\n",
    "       color='#1f77b4',\n",
    "       alpha=0.8,\n",
    "       yerr=deviation_ste_true_state,\n",
    "       capsize=5)\n",
    "\n",
    "# Define custom labels for the legend\n",
    "legend_labels = ['Sentiment', 'PbRL', 'RLHTF no instructions (ours)', 'RLHTF with instructions (ours)']\n",
    "colors=['#9467bd', '#2ca02c', '#7f7f7f', '#ff7f0e']\n",
    "\n",
    "# Plot the bars and error bars for each score type\n",
    "for i, score_type in enumerate(means):\n",
    "    adjusted_means = [10 - value for value in means[score_type]]\n",
    "    ax.bar(positions[score_type], adjusted_means, bar_width, label=legend_labels[i],\n",
    "           color=colors[i], alpha=0.8, yerr=stes[score_type], capsize=5)\n",
    "    \n",
    "\n",
    "# Labeling the plot with increased font size\n",
    "ax.set_xlabel('Interaction', fontsize=16)  # Change the fontsize as needed\n",
    "ax.set_ylabel('Average Error Â± Standard Error', fontsize=16)\n",
    "#ax.set_title('Scores by Step with Mean and STE', fontsize=16)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'{i}' for i in x], fontsize=14)  # Adjust fontsize for x-tick labels\n",
    "ax.legend(loc='lower left', fontsize=16)  # Adjust fontsize for the legend\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Optionally, you can adjust the y-tick labels font size as well\n",
    "ax.tick_params(axis='y', labelsize=12)  # Adjust fontsize for y-tick labels\n",
    "\n",
    "\n",
    "# Show grid for better readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
